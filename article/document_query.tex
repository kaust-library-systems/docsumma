\documentclass[12pt, a4paper]{article}
\usepackage{graphicx}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\addbibresource{docsumma.bib}

\title{Document Querying}
\author{Marcelo Garcia}
\date{\today}

\begin{document}
\maketitle
\section{Introduction}
Generative AI (or \emph{GenAI}) took the world by storm since the release of ChatGPT in November 2022. It was the fastest growing application in history reaching one hundred million users in just two months\cite{chatgpt_grow}. 

In this work we describe a system to help the appraisal of documents by creating a summary of each document.

The summary is generated using a local Large Language Model (LLM) called ``Ollama.'' Local because the model runs on the local computer without the need of any network access. This is important to the privacy data since it's not upload to a cloud provider. 

Indeed since no data is sent outside local computer we can provide our documents to the model and improve the relevance of the answer we get. This is technique is called ``retrieval augmented generation'' (RAG).

RAG allows generative AI models to access additional external knowledge bases, such as internal organizational data, scholarly journals and specialized datasets. By integrating relevant information into the generation process, chatbots and other natural language processing (NLP) tools can create more accurate domain-specific content without needing further training.

\printbibliography

\end{document}